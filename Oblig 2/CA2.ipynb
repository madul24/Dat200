{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CA2 - Supervised machine learning classification pipeline - applied to medical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important information\n",
    "\n",
    "- Do __not__ use scikit-learn (`sklearn`) or any other high-level machine learning library for this CA\n",
    "- Explain your code and reasoning in markdown cells or code comments\n",
    "- Label all graphs and charts if applicable\n",
    "- If you use code from the internet, make sure to reference it and explain it in your own words\n",
    "- If you use additional function arguments, make sure to explain them in your own words\n",
    "- Use the classes `Perceptron`, `Adaline` and `Logistic Regression` from the library `mlxtend` as classifiers (`from mlxtend.classifier import Perceptron, Adaline, LogisticRegression`). _Always_ use the argument `minibatches=1` when instantiating an `Adaline` or `LogisticRegression` object. This makes the model use the gradient descent algorithm for training. Always use the `random_seed=42` argument when instantiating the classifiers. This will make your results reproducible.\n",
    "- You can use any plotting library you want (e.g. `matplotlib`, `seaborn`, `plotly`, etc.)\n",
    "- Use explanatory variable names (e.g. `X_train` and `X_train_scaled` for the training data before and after scaling, respectively)\n",
    "- The dataset is provided in the file `fetal_health.csv` in the `assets` folder\n",
    "\n",
    "### Additional clues\n",
    "\n",
    "- Use the `pandas` library for initial data inspection and preprocessing\n",
    "- Before training the classifiers, convert the data to raw `numpy` arrays\n",
    "- For Part IV, you are aiming to create a plot that looks similar to this:\n",
    "<img src=\"./assets/example_output.png\" width=\"300\">\n",
    "\n",
    "### Additional information\n",
    "\n",
    "- Feel free to create additional code or markdown cells if you think it will help you explain your reasoning or structure your code (you don't have to).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Data loading and data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries/modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code below\n",
    "# ======================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and exploring data\n",
    "\n",
    "1. Load the dataset `fetal_health.csv` with `pandas`. Use the first column as the row index.\n",
    "2. Check for missing data, report on your finding and remove samples with missing data, if you find any.\n",
    "3. Display the raw data with appropriate plots/outputs and inspect it. Describe the distributions of the values of feature `\"baseline value\"`, `\"accelerations\"`, and the target variable `\"fetal_health\"`.\n",
    "4. Will it be beneficial to scale the data? Why or why not?\n",
    "5. Is the data linearly separable using a combination of any two pairs of features? Can we expect an accuracy close to 100% from a linear classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code below\n",
    "# ======================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Train/Test Split\n",
    "\n",
    "Divide your dataset into training and testing subsets. Follow these steps to create the split:\n",
    "\n",
    "1. **Divide the dataset into two data sets, each data set only contains samples of either class 0 or class 1:**\n",
    "- Create a DataFrame `df_0` containing all data with `\"fetal_health\"` equal to 0.\n",
    "- Create a DataFrame `df_1` containing all data with `\"fetal_health\"` equal to 1.\n",
    "\n",
    "2. **Split into training and test set by randomly sampling entries from the data frames:**\n",
    "- Create a DataFrame `df_0_train` containing by sampling `75%` of the entries from `df_0` (use the `sample` method of the data frame, fix the `random_state` to `42`).\n",
    "- Create a DataFrame `df_1_train` using the same approach with `df_1`.\n",
    "- Create a DataFrame `df_0_test` containing the remaining entries of `df_0` (use `df_0.drop(df_0_train.index)` to drop all entries except the previously extracted ones).\n",
    "- Create a DataFrame `df_1_test` using the same approach with `df_1`.\n",
    "\n",
    "3. **Merge the datasets split by classes back together:**\n",
    "- Create a DataFrame `df_train` containing all entries from `df_0_train` and `df_1_train`. (Hint: use the `concat` method you know from CA1)\n",
    "- Create a DataFrame `df_test` containing all entries from the two test sets.\n",
    "\n",
    "4. **Create the following data frames from these splits:**\n",
    "- `X_train`: Contains all columns of `df_train` except for the target feature `\"fetal_health\"`\n",
    "- `X_test`: Contains all columns of `df_test` except for the target feature `\"fetal_health\"`\n",
    "- `y_train`: Contains only the target feature `\"fetal_health\"` for all samples in the training set\n",
    "- `y_test`: Contains only the target feature `\"fetal_health\"` for all samples in the test set\n",
    "\n",
    "5. **Check that your sets have the expected sizes/shape by printing number of rows and colums (\"shape\") of the data sets.**\n",
    "- (Sanity check: there should be 8 features, almost 1000 samples in the training set and slightly more than 300 samples in the test set.)\n",
    "\n",
    "\n",
    "6. **Explain the purpose of this slightly complicated procedure. Why did we first split into the two classes? Why did we then split into a training and a testing set?**\n",
    "\n",
    "\n",
    "7. **What is the share (in percent) of samples with class 0 label in test and training set, and in the intial data set?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code below\n",
    "# ======================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data to numpy arrays and shuffle the training data\n",
    "\n",
    "Many machine learning models (including those you will work with later in the assignment) will not accept DataFrames as input. Instead, they will only work if you pass numpy arrays containing the data.\n",
    "Here, we convert the DataFrames `X_train`, `X_test`, `y_train`, and `y_test` to numpy arrays `X_train`, `X_test`, `y_train`, and `y_test`.\n",
    "\n",
    "Moreover we shuffle the training data. This is important because the training data is currently ordered by class. In Part IV, we use the first n samples from the training set to train the classifiers. If we did not shuffle the data, the classifiers would only be trained on samples of class 0.\n",
    "\n",
    "Nothing to be done here, just execute the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy arrays\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "# shuffle training data\n",
    "np.random.seed(42) # for reproducibility\n",
    "shuffle_index = np.random.permutation(len(X_train)) # generate random indices\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index] # shuffle data by applying reordering with the random indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Scaling the data\n",
    "\n",
    "1. Standardize the training _and_ test data so that each feature has a mean of 0 and a standard deviation of 1.\n",
    "2. Check that the scaling was successful\n",
    "    - by printing the mean and standard deviation of each feature in the scaled training set\n",
    "    - by putting the scaled training set into a DataFrame and make a violin plot of the data\n",
    "\n",
    "__Hint:__ use the `axis` argument to calculate mean and standard deviation column-wise.\n",
    "\n",
    "__Important:__ Avoid data leakage!\n",
    "\n",
    "__More hints:__\n",
    "\n",
    "1. For each column, subtract the mean $(\\mu)$ of each column from each value in the column\n",
    "2. Divide the result by the standard deviation $(\\sigma)$ of the column\n",
    "\n",
    "(You saw how to do both operations in the lecture. If you don't remember, you can look it up in Canvas files.)\n",
    "\n",
    "Mathematically (in case this is useful for you), this transformation can be represented for each column as follows:\n",
    "\n",
    "$$ X_\\text{scaled} = \\frac{(X - \\mu)}{\\sigma} $$\n",
    "\n",
    "where:\n",
    "- $(X_\\text{scaled})$ are the new, transformed column values (a column-vector)\n",
    "- $(X)$ is the original values\n",
    "- $(\\mu)$ is the mean of the column\n",
    "- $(\\sigma)$ is the standard deviation of the column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code below\n",
    "# ======================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV: Training and evaluation with different dataset sizes and training times\n",
    "\n",
    "Often, a larger dataset size will yield better model performance. (As we will learn later, this usually prevents overfitting and increases the generalization capability of the trained model.)\n",
    "However, collecting data is usually rather expensive.\n",
    "\n",
    "In this part of the exercise, you will investigate\n",
    "\n",
    "- how the model performance changes with varying dataset size\n",
    "- how the model performance changes with varying numbers of epochs/iterations of the optimizer/solver (increasing training time).\n",
    "\n",
    "For this task (Part IV), use the `Adaline`, `Perceptron`, and `LogisticRegression` classifier from the `mlxtend` library. All use the gradient descent (GD) algorithm for training.\n",
    "\n",
    "__Important__: Use a learning rate of `1e-4` (`0.0001`) for all classifiers, and use the argument `minibatches=1` when initializing `Adaline` and `LogisticRegression` classifier (this will make sure it uses GD). For all three classifiers, pass `random_seed=42` when initializing the classifier to ensure reproducibility of the results.\n",
    "\n",
    "### Model training\n",
    "\n",
    "Train the model models using progressively larger subsets of your dataset, specifically: first 50 rows, first 100 rows, first 150 rows, ..., first 650 rows, first 700 rows (in total $14$ different variants).\n",
    "\n",
    "For each number of rows train the model with progressively larger number of epochs: 2, 7, 12, 17, ..., 87, 92, 97 (in total $20$ different model variants).\n",
    "\n",
    "The resulting $14 \\times 20 = 280$ models obtained from the different combinations of subsets and number of epochs. An output of the training process could look like this:\n",
    "\n",
    "```\n",
    "Model (1) Train a model with first 50 rows of data for 2 epochs\n",
    "Model (2) Train a model with first 50 rows of data for 7 epochs\n",
    "Model (3) Train a model with first 50 rows of data for 12 epochs\n",
    "...\n",
    "Model (21) Train a model with first 100 rows of data for 2 epochs\n",
    "Model (22) Train a model with first 100 rows of data for 7 epochs\n",
    "...\n",
    "Model (279) Train a model with first 700 rows of data for 92 epochs\n",
    "Model (280) Train a model with first 700 rows of data for 97 epochs\n",
    "```\n",
    "\n",
    "### Model evaluation\n",
    "\n",
    "For each of the $280$ models, calculate the __accuracy on the test set__ (do __not__ use the score method but compute accuracy yourself).\n",
    "Store the results in the provided 2D numpy array (it has $14$ rows and $20$ columns).\n",
    "The rows of the array correspond to the different dataset sizes, and the columns correspond to the different numbers of epochs.\n",
    "\n",
    "### Tasks\n",
    "1. Train the $280$ Adaline classifiers as mentioned above and calculate the accuracy for each of the $280$ variants.\n",
    "2. Generalize your code so that is doing the same procedure for all three classifiers: `Perceptron`, `Adaline`, and `LogisticRegression` after each other. Store the result for all classifiers. You can for example use an array of shape $3\\times14\\times20$ to store the accuracies of the three classifiers.\n",
    "\n",
    "Note that executing the cells will take some time (but on most systems it should not be more than 5 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate all model variants\n",
    "# Insert your code below\n",
    "# ======================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance visualization\n",
    "\n",
    "Plot the performance measure for all classifiers (accuracy on the test set; use the result array from above) of all the $280$ variants for each classifier in a total of three heatmaps using, for example `seaborn` or `matplotlib` directly.\n",
    "\n",
    "The color should represent the accuracy on the test set, and the x and y axes should represent the number of epochs and the dataset size, respectively.\n",
    "Which one is x and which one is y is up to you to decide. Look in the example output at the top of the assignment for inspiration for how the plot could look like and how it could be labeled nicely. (But use the correct numbers corresponding to your dataset sizes and number of epochs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code below\n",
    "# ======================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V: Some more plotting\n",
    "\n",
    "For the following cell to execute you need to have the variable `X_test_scaled` with all samples of the test set and the variable `y_test` with the corresponding labels.\n",
    "Complete at least up until Part III. Executing the cell will plot something.\n",
    "\n",
    "1. Add code comments explaining what the lines are doing\n",
    "2. What is the purpose of the plot?\n",
    "3. Describe all components of the subplot and then comment in general on the entire plot. What does it show? What does it not show?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and a logistic regression model with 300 epochs and learning rate 0.0001\n",
    "clf = LogisticRegression(eta = 0.0001, epochs = 300, minibatches=1, random_seed=42)\n",
    "clf.fit(X_test_scaled, y_test)\n",
    "\n",
    "fig, axes = plt.subplots(8, 8, figsize=(30, 30))\n",
    "for i in range(0, 8):\n",
    "    for j in range(0, 8):\n",
    "        feature_1 = i\n",
    "        feature_2 = j\n",
    "        ax = axes[i, j]\n",
    "\n",
    "        ax.set_xlabel(f\"Feature {feature_1}\")\n",
    "        ax.set_ylabel(f\"Feature {feature_2}\")\n",
    "\n",
    "        mins = X_test_scaled.min(axis=0)\n",
    "        maxs = X_test_scaled.max(axis=0)\n",
    "\n",
    "        x0 = np.linspace(mins[feature_1], maxs[feature_1], 100)\n",
    "        x1 = np.linspace(mins[feature_2], maxs[feature_2], 100)\n",
    "\n",
    "        X0, X1 = np.meshgrid(x0, x1)\n",
    "        X_two_features = np.c_[X0.ravel(), X1.ravel()]\n",
    "        X_plot = np.zeros(shape=(X_two_features.shape[0], X_test_scaled.shape[1]))\n",
    "\n",
    "        X_plot[:, feature_1] = X_two_features[:, 0]\n",
    "        X_plot[:, feature_2] = X_two_features[:, 1]\n",
    "\n",
    "        y_pred = clf.predict_proba(X_plot)\n",
    "        Z = y_pred.reshape(X0.shape)\n",
    "\n",
    "        ax.pcolor(X0, X1, Z)\n",
    "        ax.contour(X0, X1, Z, levels=[0.5], colors='k')\n",
    "        ax.scatter(X_test_scaled[y_test == 0, feature_1], X_test_scaled[y_test == 0, feature_2], color=\"b\", marker=\"^\", s=50, facecolors=\"none\")\n",
    "        ax.scatter(X_test_scaled[y_test == 1, feature_1], X_test_scaled[y_test == 1, feature_2], color=\"y\", marker=\"o\", s=50, facecolors=\"none\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part VI: Additional discussion\n",
    "\n",
    "### Part I:\n",
    "1. What kind of plots did you use to visualize the raw data, and why did you choose these types of plots?\n",
    "\n",
    "### Part II:\n",
    "1. What happens if we don't shuffle the training data before training the classifiers like in Part IV?\n",
    "2. How could you do the same train/test split (Point 1.-4.) using scikit-learn?\n",
    "\n",
    "### Part IV:\n",
    "1. How does increasing the dataset size affect the performance of the logistic regression model? Provide a summary of your findings.\n",
    "2. Describe the relationship between the number of epochs and model accuracy\n",
    "3. Which classifier is much slower to train and why do you think that is?\n",
    "4. One classifier shows strong fluctuations in accuracy for different dataset sizes and number of epochs. Which one is it and why do you think this happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
